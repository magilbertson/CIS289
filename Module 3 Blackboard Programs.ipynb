{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_page(url):\n",
    "    header = {'User-agent' : 'Mozilla/5.0 (Windows; U; Windows NT 5.1; de; rv:1.9.1.5) Gecko/20091102 Firefox/3.5.5'}\n",
    "    with closing(get(url, headers=header)) as resp:\n",
    "        return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_html = get_a_page(\"https://desmoines.craigslist.org/d/recreational-vehicles/search/rva\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "html = BeautifulSoup(raw_html.text, 'html.parser')\n",
    "postings = html.find_all('li', class_= 'result-row')\n",
    "print(type(postings))\n",
    "print(len(postings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (postings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data for a single posting to ensure this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get price for first listing\n",
    "post_1_price = postings[0].a.text\n",
    "post_1_price = post_1_price.strip()\n",
    "print(post_1_price)\n",
    "# Get timestamp for first listing\n",
    "post_1_time = postings[0].find('time', class_= 'result-date')\n",
    "post_1_datetime = post_1_time['datetime']\n",
    "print(post_1_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text of title of first listing\n",
    "post_1_title = postings[0].find('a', class_= 'result-title hdrlnk')\n",
    "post_1_link = post_1_title['href']\n",
    "print( post_1_link,post_1_title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_1_location = postings[0].find(class_= 'result-hood')\n",
    "print(post_1_location.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now gather data for all listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import re\n",
    "from random import randint #avoid throttling by not sending too many requests one after the other\n",
    "from warnings import warn\n",
    "from time import time\n",
    "from IPython.core.display import clear_output\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_num = html.find('div', class_= 'search-legend')\n",
    "results_total = int(results_num.find('span', class_='totalcount').text)\n",
    "print(results_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = np.arange(0, results_total+1, 120)\n",
    "print(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 0\n",
    "\n",
    "post_timing = []\n",
    "post_hoods = []\n",
    "post_title_texts = []\n",
    "post_prices = []\n",
    "post_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    \n",
    "    #get request\n",
    "    response = get(\"https://desmoines.craigslist.org/d/recreational-vehicles/search/rva?\" \n",
    "                   + \"s=\" #the parameter for defining the page number \n",
    "                   + str(page) #the page number in the pages array from earlier\n",
    "                   )\n",
    "\n",
    "    sleep(randint(1,5))\n",
    "     \n",
    "    #throw warning for status codes that are not 200\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "        \n",
    "    #define the html text\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #define the posts\n",
    "    posts = page_html.find_all('li', class_= 'result-row')\n",
    "        \n",
    "    #extract data item-wise\n",
    "    for post in posts:\n",
    "\n",
    "        if post.find('span', class_ = 'result-hood') is not None:\n",
    "\n",
    "            #posting date\n",
    "            #grab the datetime element 0 for date and 1 for time\n",
    "            post_datetime = post.find('time', class_= 'result-date')['datetime']\n",
    "            post_timing.append(post_datetime)\n",
    "\n",
    "            #neighborhoods\n",
    "            post_hood = post.find('span', class_= 'result-hood').text\n",
    "            post_hoods.append(post_hood)\n",
    "\n",
    "            #title text\n",
    "            post_title = post.find('a', class_='result-title hdrlnk')\n",
    "            post_title_text = post_title.text\n",
    "            post_title_texts.append(post_title_text)\n",
    "\n",
    "            #post link\n",
    "            post_link = post_title['href']\n",
    "            post_links.append(post_link)\n",
    "            \n",
    "            #removes the \\n whitespace from each side, removes the currency symbol, and turns it into an int\n",
    "            post_price = int(post.a.text.strip().replace(\"$\", \"\")) \n",
    "            post_prices.append(post_price)\n",
    "            \n",
    "    iterations += 1\n",
    "    print(\"Page \" + str(iterations) + \" scraped successfully!\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Scrape complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can now load all my lists into a dictionary of lists (or whatever data structure makes the most sense)\n",
    "all_listing_dict = {}\n",
    "for listing in range (0,len(post_title_texts)):\n",
    "    all_listing_dict[listing]=[post_timing[listing],post_hoods[listing],post_title_texts[listing],post_prices[listing],post_links[listing]]\n",
    "\n",
    "print(all_listing_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### File Input Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example assumes you have the Moby_Dick_Chapter_1.txt file in the same directory as your program\n",
    "with open('Moby_Dick_Chapter_1.txt','r') as input_file:\n",
    "    for line in input_file:\n",
    "        print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example assumes you have the Moby_Dick_Chapter_1.txt file in the same directory as your program\n",
    "import re\n",
    "moby_dick_word_count = {}\n",
    "\n",
    "with open('Moby_Dick_Chapter_1.txt','r') as input_file:\n",
    "    for line in input_file:\n",
    "        # First lowercase all characters in the line\n",
    "        line = line.lower()\n",
    "        # Next clean the line of any punctuation\n",
    "        line_clean =  re.sub(r'[^\\w\\s]', '', line)\n",
    "        # Now split the line into words\n",
    "        line_split = line_clean.split()\n",
    "        # Now we can add the words to our dictionary\n",
    "        for word in line_split:\n",
    "            if word in moby_dick_word_count.keys():\n",
    "                moby_dick_word_count[word] += 1\n",
    "            else:\n",
    "                moby_dick_word_count[word] = 1\n",
    "print(moby_dick_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(moby_dick_word_count.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_output = sorted(moby_dick_word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "with open(\"moby_dick_word_counts.txt\",'w') as fileoutput:\n",
    "    for item in data_for_output:\n",
    "        #print(item)\n",
    "        fileoutput.write(item[0] + \" \" + str(item[1])+ \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_by_2_50_times(number):\n",
    "    for i in range(0,50):\n",
    "        number /= 2\n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "divide_by_2_50_times(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array('i', [5, 4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "import array\n",
    "my_array = array.array('i',[5,4,3,2])\n",
    "print(my_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array('i', [5, 5])\n",
      "array('i', [5, 5, 3, 2, 9, 9, 9])\n",
      "array('i', [5, 5, 3, 2, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "print(my_array[0:2])\n",
    "my_array[1] = 5\n",
    "my_array.append(9)\n",
    "print(my_array)\n",
    "my_array.remove(9)\n",
    "print(my_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_array.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]\n",
      " [13 14 15 16]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "my_narray = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\n",
    "print(my_narray)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
